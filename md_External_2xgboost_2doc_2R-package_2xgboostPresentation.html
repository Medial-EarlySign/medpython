<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Medial Code Documentation: XGBoost R Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Medial Code Documentation
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">XGBoost R Tutorial</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="autotoc_md171"></a>
Introduction</h1>
<p><b>XGBoost</b> is short for e**X**treme **G**radient **Boost**ing package.</p>
<p>The purpose of this Vignette is to show you how to use <b>XGBoost</b> to build a model and make predictions.</p>
<p>It is an efficient and scalable implementation of gradient boosting framework by @friedman2000additive and @friedman2001greedy. Two solvers are included:</p>
<ul>
<li><em>linear</em> model ;</li>
<li><em>tree learning</em> algorithm.</li>
</ul>
<p>It supports various objective functions, including <em>regression</em>, <em>classification</em> and <em>ranking</em>. The package is made to be extendible, so that users are also allowed to define their own objective functions easily.</p>
<p>It has been <a href="https://github.com/dmlc/xgboost">used</a> to win several <a href="http://www.kaggle.com">Kaggle</a> competitions.</p>
<p>It has several features:</p>
<ul>
<li>Speed: it can automatically do parallel computation on <em>Windows</em> and <em>Linux</em>, with <em>OpenMP</em>. It is generally over 10 times faster than the classical <code>gbm</code>.</li>
<li>Input Type: it takes several types of input data:<ul>
<li><em>Dense</em> Matrix: <em>R</em>'s <em>dense</em> matrix, i.e. <code>matrix</code> ;</li>
<li><em>Sparse</em> Matrix: <em>R</em>'s <em>sparse</em> matrix, i.e. <code>Matrix::dgCMatrix</code> ;</li>
<li>Data File: local data files ;</li>
<li><code>xgb.DMatrix</code>: its own class (recommended).</li>
</ul>
</li>
<li>Sparsity: it accepts <em>sparse</em> input for both <em>tree booster</em> and <em>linear booster</em>, and is optimized for <em>sparse</em> input ;</li>
<li>Customization: it supports customized objective functions and evaluation functions.</li>
</ul>
<h1><a class="anchor" id="autotoc_md172"></a>
Installation</h1>
<h2><a class="anchor" id="autotoc_md173"></a>
GitHub version</h2>
<p>For weekly updated version (highly recommended), install from <em>GitHub</em>:</p>
<div class="fragment"><div class="line">install.packages(&quot;drat&quot;, repos=&quot;https://cran.rstudio.com&quot;)</div>
<div class="line">drat:::addRepo(&quot;dmlc&quot;)</div>
<div class="line">install.packages(&quot;xgboost&quot;, repos=&quot;http://dmlc.ml/drat/&quot;, type = &quot;source&quot;)</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;<em>Windows</em> users will need to install <a href="http://cran.r-project.org/bin/windows/Rtools/">Rtools</a> first. </p>
</blockquote>
<h2><a class="anchor" id="autotoc_md174"></a>
CRAN version</h2>
<p>The version 0.4-2 is on CRAN, and you can install it by:</p>
<div class="fragment"><div class="line">install.packages(&quot;xgboost&quot;)</div>
</div><!-- fragment --><p>Formerly available versions can be obtained from the CRAN <a href="http://cran.r-project.org/src/contrib/Archive/xgboost">archive</a></p>
<h1><a class="anchor" id="autotoc_md175"></a>
Learning</h1>
<p>For the purpose of this tutorial we will load <b>XGBoost</b> package.</p>
<div class="fragment"><div class="line">require(xgboost)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md176"></a>
Dataset presentation</h2>
<p>In this example, we are aiming to predict whether a mushroom can be eaten or not (like in many tutorials, example data are the same as you will use on in your every day life :-).</p>
<p>Mushroom data is cited from UCI Machine Learning Repository. @Bache+Lichman:2013.</p>
<h2><a class="anchor" id="autotoc_md177"></a>
Dataset loading</h2>
<p>We will load the <code>agaricus</code> datasets embedded with the package and will link them to variables.</p>
<p>The datasets are already split in:</p>
<ul>
<li><code>train</code>: will be used to build the model ;</li>
<li><code>test</code>: will be used to assess the quality of our model.</li>
</ul>
<p>Why <em>split</em> the dataset in two parts?</p>
<p>In the first part we will build our model. In the second part we will want to test it and assess its quality. Without dividing the dataset we would test the model on the data which the algorithm have already seen.</p>
<div class="fragment"><div class="line">data(agaricus.train, package=&#39;xgboost&#39;)</div>
<div class="line">data(agaricus.test, package=&#39;xgboost&#39;)</div>
<div class="line">train &lt;- agaricus.train</div>
<div class="line">test &lt;- agaricus.test</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;In the real world, it would be up to you to make this division between <code>train</code> and <code>test</code> data. The way to do it is out of scope for this article, however <code>caret</code> package may <a href="http://topepo.github.io/caret/data-splitting.html">help</a>. </p>
</blockquote>
<p>Each variable is a <code>list</code> containing two things, <code>label</code> and <code>data</code>:</p>
<div class="fragment"><div class="line">str(train)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## List of 2</div>
<div class="line">##  $ data :Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots</div>
<div class="line">##   .. ..@ i       : int [1:143286] 2 6 8 11 18 20 21 24 28 32 ...</div>
<div class="line">##   .. ..@ p       : int [1:127] 0 369 372 3306 5845 6489 6513 8380 8384 10991 ...</div>
<div class="line">##   .. ..@ Dim     : int [1:2] 6513 126</div>
<div class="line">##   .. ..@ Dimnames:List of 2</div>
<div class="line">##   .. .. ..$ : NULL</div>
<div class="line">##   .. .. ..$ : chr [1:126] &quot;cap-shape=bell&quot; &quot;cap-shape=conical&quot; &quot;cap-shape=convex&quot; &quot;cap-shape=flat&quot; ...</div>
<div class="line">##   .. ..@ x       : num [1:143286] 1 1 1 1 1 1 1 1 1 1 ...</div>
<div class="line">##   .. ..@ factors : list()</div>
<div class="line">##  $ label: num [1:6513] 1 0 0 1 0 0 0 1 0 0 ...</div>
</div><!-- fragment --><p><code>label</code> is the outcome of our dataset meaning it is the binary <em>classification</em> we will try to predict.</p>
<p>Let's discover the dimensionality of our datasets.</p>
<div class="fragment"><div class="line">dim(train$data)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] 6513  126</div>
</div><!-- fragment --><div class="fragment"><div class="line">dim(test$data)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] 1611  126</div>
</div><!-- fragment --><p>This dataset is very small to not make the <b>R</b> package too heavy, however <b>XGBoost</b> is built to manage huge datasets very efficiently.</p>
<p>As seen below, the <code>data</code> are stored in a <code>dgCMatrix</code> which is a <em>sparse</em> matrix and <code>label</code> vector is a <code>numeric</code> vector (<code>{0,1}</code>):</p>
<div class="fragment"><div class="line">class(train$data)[1]</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;dgCMatrix&quot;</div>
</div><!-- fragment --><div class="fragment"><div class="line">class(train$label)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;numeric&quot;</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md178"></a>
Basic Training using XGBoost</h2>
<p>This step is the most critical part of the process for the quality of our model.</p>
<h3><a class="anchor" id="autotoc_md179"></a>
Basic training</h3>
<p>We are using the <code>train</code> data. As explained above, both <code>data</code> and <code>label</code> are stored in a <code>list</code>.</p>
<p>In a <em>sparse</em> matrix, cells containing <code>0</code> are not stored in memory. Therefore, in a dataset mainly made of <code>0</code>, memory size is reduced. It is very common to have such a dataset.</p>
<p>We will train decision tree model using the following parameters:</p>
<ul>
<li><code>objective = "binary:logistic"</code>: we will train a binary classification model ;</li>
<li><code>max.depth = 2</code>: the trees won't be deep, because our case is very simple ;</li>
<li><code>nthread = 2</code>: the number of CPU threads we are going to use;</li>
<li><code>nrounds = 2</code>: there will be two passes on the data, the second one will enhance the model by further reducing the difference between ground truth and prediction.</li>
</ul>
<div class="fragment"><div class="line">bstSparse &lt;- xgboost(data = train$data, label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522</div>
<div class="line">## [1]  train-error:0.022263</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;The more complex the relationship between your features and your <code>label</code> is, the more passes you need. </p>
</blockquote>
<h3><a class="anchor" id="autotoc_md180"></a>
Parameter variations</h3>
<h4><a class="anchor" id="autotoc_md181"></a>
Dense matrix</h4>
<p>Alternatively, you can put your dataset in a <em>dense</em> matrix, i.e. a basic <b>R</b> matrix.</p>
<div class="fragment"><div class="line">bstDense &lt;- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522</div>
<div class="line">## [1]  train-error:0.022263</div>
</div><!-- fragment --><h4><a class="anchor" id="autotoc_md182"></a>
xgb.DMatrix</h4>
<p><b>XGBoost</b> offers a way to group them in a <code>xgb.DMatrix</code>. You can even add other meta data in it. This will be useful for the most advanced features we will discover later.</p>
<div class="fragment"><div class="line">dtrain &lt;- xgb.DMatrix(data = train$data, label = train$label)</div>
<div class="line">bstDMatrix &lt;- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522</div>
<div class="line">## [1]  train-error:0.022263</div>
</div><!-- fragment --><h4><a class="anchor" id="autotoc_md183"></a>
Verbose option</h4>
<p><b>XGBoost</b> has several features to help you view the learning progress internally. The purpose is to help you to set the best parameters, which is the key of your model quality.</p>
<p>One of the simplest way to see the training progress is to set the <code>verbose</code> option (see below for more advanced techniques).</p>
<div class="fragment"><div class="line"># verbose = 0, no message</div>
<div class="line">bst &lt;- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;, verbose = 0)</div>
</div><!-- fragment --><div class="fragment"><div class="line"># verbose = 1, print evaluation metric</div>
<div class="line">bst &lt;- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;, verbose = 1)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522</div>
<div class="line">## [1]  train-error:0.022263</div>
</div><!-- fragment --><div class="fragment"><div class="line"># verbose = 2, also print information about tree</div>
<div class="line">bst &lt;- xgboost(data = dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = &quot;binary:logistic&quot;, verbose = 2)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [11:41:01] amalgamation/../src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2</div>
<div class="line">## [0]  train-error:0.046522</div>
<div class="line">## [11:41:01] amalgamation/../src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2</div>
<div class="line">## [1]  train-error:0.022263</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md184"></a>
Basic prediction using XGBoost</h1>
<h1><a class="anchor" id="autotoc_md185"></a>
Perform the prediction</h1>
<p>The purpose of the model we have built is to classify new data. As explained before, we will use the <code>test</code> dataset for this step.</p>
<div class="fragment"><div class="line">pred &lt;- predict(bst, test$data)</div>
<div class="line"> </div>
<div class="line"># size of the prediction vector</div>
<div class="line">print(length(pred))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] 1611</div>
</div><!-- fragment --><div class="fragment"><div class="line"># limit display of predictions to the first 10</div>
<div class="line">print(head(pred))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] 0.28583017 0.92392391 0.28583017 0.28583017 0.05169873 0.92392391</div>
</div><!-- fragment --><p>These numbers doesn't look like <em>binary classification</em> <code>{0,1}</code>. We need to perform a simple transformation before being able to use these results.</p>
<h1><a class="anchor" id="autotoc_md186"></a>
Transform the regression in a binary classification</h1>
<p>The only thing that <b>XGBoost</b> does is a <em>regression</em>. <b>XGBoost</b> is using <code>label</code> vector to build its <em>regression</em> model.</p>
<p>How can we use a <em>regression</em> model to perform a binary classification?</p>
<p>If we think about the meaning of a regression applied to our data, the numbers we get are probabilities that a datum will be classified as <code>1</code>. Therefore, we will set the rule that if this probability for a specific datum is <code>&gt; 0.5</code> then the observation is classified as <code>1</code> (or <code>0</code> otherwise).</p>
<div class="fragment"><div class="line">prediction &lt;- as.numeric(pred &gt; 0.5)</div>
<div class="line">print(head(prediction))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] 0 1 0 0 0 1</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md187"></a>
Measuring model performance</h1>
<p>To measure the model performance, we will compute a simple metric, the <em>average error</em>.</p>
<div class="fragment"><div class="line">err &lt;- mean(as.numeric(pred &gt; 0.5) != test$label)</div>
<div class="line">print(paste(&quot;test-error=&quot;, err))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;test-error= 0.0217256362507759&quot;</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;Note that the algorithm has not seen the <code>test</code> data during the model construction. </p>
</blockquote>
<p>Steps explanation:</p>
<ol type="1">
<li><code>as.numeric(pred &gt; 0.5)</code> applies our rule that when the probability (&lt;=&gt; regression &lt;=&gt; prediction) is <code>&gt; 0.5</code> the observation is classified as <code>1</code> and <code>0</code> otherwise ;</li>
<li><code>probabilityVectorPreviouslyComputed != test$label</code> computes the vector of error between true data and computed probabilities ;</li>
<li><code>mean(vectorOfErrors)</code> computes the <em>average error</em> itself.</li>
</ol>
<p>The most important thing to remember is that <b>to do a classification, you just do a regression to the</b> <code>label</code> <b>and then apply a threshold</b>.</p>
<p><em>Multiclass</em> classification works in a similar way.</p>
<p>This metric is <b>0.02</b> and is pretty low: our yummly mushroom model works well!</p>
<h1><a class="anchor" id="autotoc_md188"></a>
Advanced features</h1>
<p>Most of the features below have been implemented to help you to improve your model by offering a better understanding of its content.</p>
<h2><a class="anchor" id="autotoc_md189"></a>
Dataset preparation</h2>
<p>For the following advanced features, we need to put data in <code>xgb.DMatrix</code> as explained above.</p>
<div class="fragment"><div class="line">dtrain &lt;- xgb.DMatrix(data = train$data, label=train$label)</div>
<div class="line">dtest &lt;- xgb.DMatrix(data = test$data, label=test$label)</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md190"></a>
Measure learning progress with xgb.train</h2>
<p>Both <code>xgboost</code> (simple) and <code>xgb.train</code> (advanced) functions train models.</p>
<p>One of the special features of <code>xgb.train</code> is the capacity to follow the progress of the learning after each round. Because of the way boosting works, there is a time when having too many rounds lead to overfitting. You can see this feature as a cousin of a cross-validation method. The following techniques will help you to avoid overfitting or optimizing the learning time in stopping it as soon as possible.</p>
<p>One way to measure progress in the learning of a model is to provide to <b>XGBoost</b> a second dataset already classified. Therefore it can learn on the first dataset and test its model on the second one. Some metrics are measured after each round during the learning.</p>
<blockquote class="doxtable">
<p>&zwj;in some way it is similar to what we have done above with the average error. The main difference is that above it was after building the model, and now it is during the construction that we measure errors. </p>
</blockquote>
<p>For the purpose of this example, we use <code>watchlist</code> parameter. It is a list of <code>xgb.DMatrix</code>, each of them tagged with a name.</p>
<div class="fragment"><div class="line">watchlist &lt;- list(train=dtrain, test=dtest)</div>
<div class="line"> </div>
<div class="line">bst &lt;- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522    test-error:0.042831</div>
<div class="line">## [1]  train-error:0.022263    test-error:0.021726</div>
</div><!-- fragment --><p><b>XGBoost</b> has computed at each round the same average error metric seen above (we set <code>nrounds</code> to 2, that is why we have two lines). Obviously, the <code>train-error</code> number is related to the training dataset (the one the algorithm learns from) and the <code>test-error</code> number to the test dataset.</p>
<p>Both training and test error related metrics are very similar, and in some way, it makes sense: what we have learned from the training dataset matches the observations from the test dataset.</p>
<p>If with your own dataset you do not have such results, you should think about how you divided your dataset in training and test. May be there is something to fix. Again, <code>caret</code> package may <a href="http://topepo.github.io/caret/data-splitting.html">help</a>.</p>
<p>For a better understanding of the learning progression, you may want to have some specific metric or even use multiple evaluation metrics.</p>
<div class="fragment"><div class="line">bst &lt;- xgb.train(data=dtrain, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, eval.metric = &quot;error&quot;, eval.metric = &quot;logloss&quot;, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522    train-logloss:0.233376  test-error:0.042831 test-logloss:0.226686</div>
<div class="line">## [1]  train-error:0.022263    train-logloss:0.136658  test-error:0.021726 test-logloss:0.137874</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;<code>eval.metric</code> allows us to monitor two new metrics for each round, <code>logloss</code> and <code>error</code>. </p>
</blockquote>
<h2><a class="anchor" id="autotoc_md191"></a>
Linear boosting</h2>
<p>Until now, all the learnings we have performed were based on boosting trees. <b>XGBoost</b> implements a second algorithm, based on linear boosting. The only difference with the previous command is <code>booster = "gblinear"</code> parameter (and removing <code>eta</code> parameter).</p>
<div class="fragment"><div class="line">bst &lt;- xgb.train(data=dtrain, booster = &quot;gblinear&quot;, nthread = 2, nrounds=2, watchlist=watchlist, eval.metric = &quot;error&quot;, eval.metric = &quot;logloss&quot;, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.024720    train-logloss:0.184616  test-error:0.022967 test-logloss:0.184234</div>
<div class="line">## [1]  train-error:0.004146    train-logloss:0.069885  test-error:0.003724 test-logloss:0.068081</div>
</div><!-- fragment --><p>In this specific case, <em>linear boosting</em> gets slightly better performance metrics than a decision tree based algorithm.</p>
<p>In simple cases, this will happen because there is nothing better than a linear algorithm to catch a linear link. However, decision trees are much better to catch a non linear link between predictors and outcome. Because there is no silver bullet, we advise you to check both algorithms with your own datasets to have an idea of what to use.</p>
<h2><a class="anchor" id="autotoc_md192"></a>
Manipulating xgb.DMatrix</h2>
<h3><a class="anchor" id="autotoc_md193"></a>
Save / Load</h3>
<p>Like saving models, <code>xgb.DMatrix</code> object (which groups both dataset and outcome) can also be saved using <code>xgb.DMatrix.save</code> function.</p>
<div class="fragment"><div class="line">xgb.DMatrix.save(dtrain, &quot;dtrain.buffer&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] TRUE</div>
</div><!-- fragment --><div class="fragment"><div class="line"># to load it in, simply call xgb.DMatrix</div>
<div class="line">dtrain2 &lt;- xgb.DMatrix(&quot;dtrain.buffer&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [11:41:01] 6513x126 matrix with 143286 entries loaded from dtrain.buffer</div>
</div><!-- fragment --><div class="fragment"><div class="line">bst &lt;- xgb.train(data=dtrain2, max.depth=2, eta=1, nthread = 2, nrounds=2, watchlist=watchlist, objective = &quot;binary:logistic&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [0]  train-error:0.046522    test-error:0.042831</div>
<div class="line">## [1]  train-error:0.022263    test-error:0.021726</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md194"></a>
Information extraction</h3>
<p>Information can be extracted from an <code>xgb.DMatrix</code> using <code>getinfo</code> function. Hereafter we will extract <code>label</code> data.</p>
<div class="fragment"><div class="line">label = getinfo(dtest, &quot;label&quot;)</div>
<div class="line">pred &lt;- predict(bst, dtest)</div>
<div class="line">err &lt;- as.numeric(sum(as.integer(pred &gt; 0.5) != label))/length(label)</div>
<div class="line">print(paste(&quot;test-error=&quot;, err))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;test-error= 0.0217256362507759&quot;</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md195"></a>
View feature importance/influence from the learnt model</h2>
<p>Feature importance is similar to R gbm package's relative influence (rel.inf).</p>
<div class="fragment"><div class="line">importance_matrix &lt;- xgb.importance(model = bst)</div>
<div class="line">print(importance_matrix)</div>
<div class="line">xgb.plot.importance(importance_matrix = importance_matrix)</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md196"></a>
View the trees from a model</h3>
<p>You can dump the tree you learned using <code>xgb.dump</code> into a text file.</p>
<div class="fragment"><div class="line">xgb.dump(bst, with_stats = TRUE)</div>
</div><!-- fragment --><div class="fragment"><div class="line">##  [1] &quot;booster[0]&quot;</div>
<div class="line">##  [2] &quot;0:[f28&lt;-1.00136e-05] yes=1,no=2,missing=1,gain=4000.53,cover=1628.25&quot;</div>
<div class="line">##  [3] &quot;1:[f55&lt;-1.00136e-05] yes=3,no=4,missing=3,gain=1158.21,cover=924.5&quot;</div>
<div class="line">##  [4] &quot;3:leaf=1.71218,cover=812&quot;</div>
<div class="line">##  [5] &quot;4:leaf=-1.70044,cover=112.5&quot;</div>
<div class="line">##  [6] &quot;2:[f108&lt;-1.00136e-05] yes=5,no=6,missing=5,gain=198.174,cover=703.75&quot;</div>
<div class="line">##  [7] &quot;5:leaf=-1.94071,cover=690.5&quot;</div>
<div class="line">##  [8] &quot;6:leaf=1.85965,cover=13.25&quot;</div>
<div class="line">##  [9] &quot;booster[1]&quot;</div>
<div class="line">## [10] &quot;0:[f59&lt;-1.00136e-05] yes=1,no=2,missing=1,gain=832.545,cover=788.852&quot;</div>
<div class="line">## [11] &quot;1:[f28&lt;-1.00136e-05] yes=3,no=4,missing=3,gain=569.725,cover=768.39&quot;</div>
<div class="line">## [12] &quot;3:leaf=0.784718,cover=458.937&quot;</div>
<div class="line">## [13] &quot;4:leaf=-0.96853,cover=309.453&quot;</div>
<div class="line">## [14] &quot;2:leaf=-6.23624,cover=20.4624&quot;</div>
</div><!-- fragment --><p>You can plot the trees from your model using `<code>xgb.plot.tree</code></p>
<div class="fragment"><div class="line">xgb.plot.tree(model = bst)</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;if you provide a path to <code>fname</code> parameter you can save the trees to your hard drive. </p>
</blockquote>
<h3><a class="anchor" id="autotoc_md197"></a>
Save and load models</h3>
<p>Maybe your dataset is big, and it takes time to train a model on it? May be you are not a big fan of losing time in redoing the same task again and again? In these very rare cases, you will want to save your model and load it when required.</p>
<p>Helpfully for you, <b>XGBoost</b> implements such functions.</p>
<div class="fragment"><div class="line"># save model to binary local file</div>
<div class="line">xgb.save(bst, &quot;xgboost.model&quot;)</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] TRUE</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;<code>xgb.save</code> function should return TRUE if everything goes well and crashes otherwise. </p>
</blockquote>
<p>An interesting test to see how identical our saved model is to the original one would be to compare the two predictions.</p>
<div class="fragment"><div class="line"># load binary model to R</div>
<div class="line">bst2 &lt;- xgb.load(&quot;xgboost.model&quot;)</div>
<div class="line">pred2 &lt;- predict(bst2, test$data)</div>
<div class="line"> </div>
<div class="line"># And now the test</div>
<div class="line">print(paste(&quot;sum(abs(pred2-pred))=&quot;, sum(abs(pred2-pred))))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;sum(abs(pred2-pred))= 0&quot;</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;result is <code>0</code>? We are good! </p>
</blockquote>
<p>In some very specific cases, like when you want to pilot <b>XGBoost</b> from <code>caret</code> package, you will want to save the model as a <em>R</em> binary vector. See below how to do it.</p>
<div class="fragment"><div class="line"># save model to R&#39;s raw vector</div>
<div class="line">rawVec &lt;- xgb.save.raw(bst)</div>
<div class="line"> </div>
<div class="line"># print class</div>
<div class="line">print(class(rawVec))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;raw&quot;</div>
</div><!-- fragment --><div class="fragment"><div class="line"># load binary model to R</div>
<div class="line">bst3 &lt;- xgb.load(rawVec)</div>
<div class="line">pred3 &lt;- predict(bst3, test$data)</div>
<div class="line"> </div>
<div class="line"># pred3 should be identical to pred</div>
<div class="line">print(paste(&quot;sum(abs(pred3-pred))=&quot;, sum(abs(pred3-pred))))</div>
</div><!-- fragment --><div class="fragment"><div class="line">## [1] &quot;sum(abs(pred3-pred))= 0&quot;</div>
</div><!-- fragment --><blockquote class="doxtable">
<p>&zwj;Again <code>0</code>? It seems that <code>XGBoost</code> works pretty well! </p>
</blockquote>
<h1><a class="anchor" id="autotoc_md198"></a>
References</h1>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Tue Nov 25 2025 14:57:48 for Medial Code Documentation by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
