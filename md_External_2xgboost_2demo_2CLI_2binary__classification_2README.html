<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Medial Code Documentation: Binary Classification</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Medial Code Documentation
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Binary Classification</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This is the quick start tutorial for xgboost CLI version. Here we demonstrate how to use XGBoost for a binary classification task. Before getting started, make sure you compile xgboost in the root directory of the project by typing <code>make</code>. The script 'runexp.sh' can be used to run the demo. Here we use <a href="https://archive.ics.uci.edu/ml/datasets/Mushroom">mushroom dataset</a> from UCI machine learning repository.</p>
<h2><a class="anchor" id="autotoc_md93"></a>
Tutorial</h2>
<h3><a class="anchor" id="autotoc_md94"></a>
Generate Input Data</h3>
<p>XGBoost takes LIBSVM format. An example of faked input data is below: </p><div class="fragment"><div class="line">1 101:1.2 102:0.03</div>
<div class="line">0 1:2.1 10001:300 10002:400</div>
<div class="line">...</div>
</div><!-- fragment --><p> Each line represent a single instance, and in the first line '1' is the instance label,'101' and '102' are feature indices, '1.2' and '0.03' are feature values. In the binary classification case, '1' is used to indicate positive samples, and '0' is used to indicate negative samples. We also support probability values in [0,1] as label, to indicate the probability of the instance being positive.</p>
<p>First we will transform the dataset into classic LIBSVM format and split the data into training set and test set by running: </p><div class="fragment"><div class="line">python mapfeat.py</div>
<div class="line">python mknfold.py agaricus.txt 1</div>
</div><!-- fragment --><p> The two files, 'agaricus.txt.train' and 'agaricus.txt.test' will be used as training set and test set.</p>
<h3><a class="anchor" id="autotoc_md95"></a>
Training</h3>
<p>Then we can run the training process: </p><div class="fragment"><div class="line">../../xgboost mushroom.conf</div>
</div><!-- fragment --><p>mushroom.conf is the configuration for both training and testing. Each line containing the [attribute]=[value] configuration:</p>
<div class="fragment"><div class="line"># General Parameters, see comment for each definition</div>
<div class="line"># can be gbtree or gblinear</div>
<div class="line">booster = gbtree</div>
<div class="line"># choose logistic regression loss function for binary classification</div>
<div class="line">objective = binary:logistic</div>
<div class="line"> </div>
<div class="line"># Tree Booster Parameters</div>
<div class="line"># step size shrinkage</div>
<div class="line">eta = 1.0</div>
<div class="line"># minimum loss reduction required to make a further partition</div>
<div class="line">gamma = 1.0</div>
<div class="line"># minimum sum of instance weight(hessian) needed in a child</div>
<div class="line">min_child_weight = 1</div>
<div class="line"># maximum depth of a tree</div>
<div class="line">max_depth = 3</div>
<div class="line"> </div>
<div class="line"># Task Parameters</div>
<div class="line"># the number of round to do boosting</div>
<div class="line">num_round = 2</div>
<div class="line"># 0 means do not save any model except the final round model</div>
<div class="line">save_period = 0</div>
<div class="line"># The path of training data</div>
<div class="line">data = &quot;agaricus.txt.train&quot;</div>
<div class="line"># The path of validation data, used to monitor training process, here [test] sets name of the validation set</div>
<div class="line">eval[test] = &quot;agaricus.txt.test&quot;</div>
<div class="line"># The path of test data</div>
<div class="line">test:data = &quot;agaricus.txt.test&quot;</div>
</div><!-- fragment --><p> We use the tree booster and logistic regression objective in our setting. This indicates that we accomplish our task using classic gradient boosting regression tree(GBRT), which is a promising method for binary classification.</p>
<p>The parameters shown in the example gives the most common ones that are needed to use xgboost. If you are interested in more parameter settings, the complete parameter settings and detailed descriptions are <a href="https://xgboost.readthedocs.io/en/stable/parameter.html">here</a>. Besides putting the parameters in the configuration file, we can set them by passing them as arguments as below:</p>
<div class="fragment"><div class="line">../../xgboost mushroom.conf max_depth=6</div>
</div><!-- fragment --><p> This means that the parameter max_depth will be set as 6 rather than 3 in the conf file. When you use command line, make sure max_depth=6 is passed in as single argument, i.e. do not contain space in the argument. When a parameter setting is provided in both command line input and the config file, the command line setting will override the setting in config file.</p>
<p>In this example, we use tree booster for gradient boosting. If you would like to use linear booster for regression, you can keep all the parameters except booster and the tree booster parameters as below: </p><div class="fragment"><div class="line"># General Parameters</div>
<div class="line"># choose the linear booster</div>
<div class="line">booster = gblinear</div>
<div class="line">...</div>
<div class="line"> </div>
<div class="line"># Change Tree Booster Parameters into Linear Booster Parameters</div>
<div class="line"># L2 regularization term on weights, default 0</div>
<div class="line">lambda = 0.01</div>
<div class="line"># L1 regularization term on weights, default 0</div>
<div class="line">alpha = 0.01</div>
<div class="line"># L2 regularization term on bias, default 0</div>
<div class="line">lambda_bias = 0.01</div>
<div class="line"> </div>
<div class="line"># Regression Parameters</div>
<div class="line">...</div>
</div><!-- fragment --><h3><a class="anchor" id="autotoc_md96"></a>
Get Predictions</h3>
<p>After training, we can use the output model to get the prediction of the test data: </p><div class="fragment"><div class="line">../../xgboost mushroom.conf task=pred model_in=0002.model</div>
</div><!-- fragment --><p> For binary classification, the output predictions are probability confidence scores in [0,1], corresponds to the probability of the label to be positive.</p>
<h3><a class="anchor" id="autotoc_md97"></a>
Dump Model</h3>
<p>This is a preliminary feature, so only tree models support text dump. XGBoost can display the tree models in text or JSON files, and we can scan the model in an easy way: </p><div class="fragment"><div class="line">../../xgboost mushroom.conf task=dump model_in=0002.model name_dump=dump.raw.txt</div>
<div class="line">../../xgboost mushroom.conf task=dump model_in=0002.model fmap=featmap.txt name_dump=dump.nice.txt</div>
</div><!-- fragment --><p>In this demo, the tree boosters obtained will be printed in dump.raw.txt and dump.nice.txt, and the latter one is easier to understand because of usage of feature mapping featmap.txt</p>
<p>Format of <code>featmap.txt: &lt;featureid&gt; &lt;featurename&gt; &lt;q or i or int&gt;\n</code>:</p><ul>
<li>Feature id must be from 0 to number of features, in sorted order.</li>
<li>i means this feature is binary indicator feature</li>
<li>q means this feature is a quantitative value, such as age, time, can be missing</li>
<li>int means this feature is integer value (when int is hinted, the decision boundary will be integer)</li>
</ul>
<h3><a class="anchor" id="autotoc_md98"></a>
Monitoring Progress</h3>
<p>When you run training we can find there are messages displayed on screen </p><div class="fragment"><div class="line">tree train end, 1 roots, 12 extra nodes, 0 pruned nodes ,max_depth=3</div>
<div class="line">[0]  test-error:0.016139</div>
<div class="line">boosting round 1, 0 sec elapsed</div>
<div class="line"> </div>
<div class="line">tree train end, 1 roots, 10 extra nodes, 0 pruned nodes ,max_depth=3</div>
<div class="line">[1]  test-error:0.000000</div>
</div><!-- fragment --><p> The messages for evaluation are printed into stderr, so if you want only to log the evaluation progress, simply type </p><div class="fragment"><div class="line">../../xgboost mushroom.conf 2&gt;log.txt</div>
</div><!-- fragment --><p> Then you can find the following content in log.txt </p><div class="fragment"><div class="line">[0]     test-error:0.016139</div>
<div class="line">[1]     test-error:0.000000</div>
</div><!-- fragment --><p> We can also monitor both training and test statistics, by adding following lines to configure </p><div class="fragment"><div class="line">eval[test] = &quot;agaricus.txt.test&quot;</div>
<div class="line">eval[trainname] = &quot;agaricus.txt.train&quot;</div>
</div><!-- fragment --><p> Run the command again, we can find the log file becomes </p><div class="fragment"><div class="line">[0]     test-error:0.016139     trainname-error:0.014433</div>
<div class="line">[1]     test-error:0.000000     trainname-error:0.001228</div>
</div><!-- fragment --><p> The rule is eval[name-printed-in-log] = filename, then the file will be added to monitoring process, and evaluated each round.</p>
<p>xgboost also supports monitoring multiple metrics, suppose we also want to monitor average log-likelihood of each prediction during training, simply add <code>eval_metric=logloss</code> to configure. Run again, we can find the log file becomes </p><div class="fragment"><div class="line">[0]     test-error:0.016139     test-negllik:0.029795   trainname-error:0.014433        trainname-negllik:0.027023</div>
<div class="line">[1]     test-error:0.000000     test-negllik:0.000000   trainname-error:0.001228        trainname-negllik:0.002457</div>
</div><!-- fragment --> <h2><a class="anchor" id="autotoc_md99"></a>
Saving Progress Models</h2>
<p>If you want to save model every two round, simply set save_period=2. You will find 0002.model in the current folder. If you want to change the output folder of models, add model_dir=foldername. By default xgboost saves the model of last round.</p>
<h3><a class="anchor" id="autotoc_md100"></a>
Continue from Existing Model</h3>
<p>If you want to continue boosting from existing model, say 0002.model, use </p><div class="fragment"><div class="line">../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model</div>
</div><!-- fragment --><p> xgboost will load from 0002.model continue boosting for 2 rounds, and save output to continue.model. However, beware that the training and evaluation data specified in mushroom.conf should not change when you use this function. </p>
<h3><a class="anchor" id="autotoc_md101"></a>
Use Multi-Threading</h3>
<p>When you are working with a large dataset, you may want to take advantage of parallelism. If your compiler supports OpenMP, xgboost is naturally multi-threaded, to set number of parallel running add <code>nthread</code> parameter to your configuration. Eg. <code>nthread=10</code></p>
<p>Set nthread to be the number of your real cpu (On Unix, this can be found using <code>lscpu</code>) Some systems will have <code>Thread(s) per core = 2</code>, for example, a 4 core cpu with 8 threads, in such case set <code>nthread=4</code> and not 8. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed Nov 5 2025 13:00:03 for Medial Code Documentation by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
